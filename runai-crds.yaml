apiVersion: v2
name: runai-cluster
description: Helm chart for installing runai cluster
version: CHART_VERSION
kubeVersion: '>=1.19.0-0'

dependencies:
  - name: runai-operator
    repository: file://./subcharts/runai-operator
    version: 1.0.0
    condition: runai-operator.enabled
  - name: gpu-feature-discovery
    repository: https://nvidia.github.io/gpu-feature-discovery
    version: 0.4.1
    condition: gpu-feature-discovery.enabled
  - name: kube-prometheus-stack
    repository: https://prometheus-community.github.io/helm-charts
    version: 18.0.5
    condition: kube-prometheus-stack.enabled
  - name: ingress-nginx
    repository: https://kubernetes.github.io/ingress-nginx
    condition: ingress-nginx.enabled
    version: 4.0.1
  - name: mpi-operator
    repository: file://./subcharts/mpi-operator
    version: 1.0.0
    condition: mpi-operator.enabled
  - name: cert-manager
    version: 1.7.0
    repository: file://./subcharts/cert-manager
    condition: cert-manager.enabled
---
createRbac:
  base: true

runai-operator:
  enabled: true

gpu-feature-discovery:
  enabled: true
  migStrategy: "mixed"
  affinity: 
    nodeAffinity: 
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.deploy.gpu-feature-discovery
            operator: NotIn
            values:
            - "false"
  nfd:
    deploy: true
  node-feature-discovery:
    master:
      extraLabelNs:
        - nvidia.com
        - run.ai 
    namespace:
      create: true

mpi-operator:
  enabled: false

ingress-nginx:
  enabled: false

cert-manager:
  enabled: false
  installCRDs: true
  namespaceOverride: "cert-manager"


kube-prometheus-stack:
  enabled: true
  namespaceOverride: monitoring

  prometheusOperator:
    enabled: true

  additionalPrometheusRulesMap:
    runai:
      groups:
        - name: runai-rules
          rules:
            - expr: (sum(runai_pod_group_info) by (pod_group_uuid, detailed_status, node_name, node, queue_name, user, workload_name, workload_type, job_name, project, job_type, job_uuid))
                * on (pod_group_uuid) group_right(detailed_status, node_name, node, queue_name, user, workload_name, workload_type, job_name, project, job_type, job_uuid) (sum(runai_pod_group_phase) by (pod_group_uuid, job_uuid, phase, status))
              record: runai_pod_group_phase_with_info
            - expr: runai_pod_info * on (pod_uuid) group_right(queue_name, pod_group_name, workload_name, workload_type, node_name, node, gpu_index, gpu, job_name, job_uuid, job_type, node, project) (sum(runai_pod_phase) by (pod_uuid, phase, status, pod_namespace, pod_name, pod_group_uuid, job_uuid))
              record: runai_pod_phase_with_info
            - expr: count by (instance, gpu) (DCGM_FI_DEV_GPU_UTIL{pod_name=~".+"} and on(pod_name, pod_namespace) (runai_pod_phase{phase="Running"} == 1))
              record: runai_gpus_running_with_pod
            - expr: runai_gpus_running_with_pod or ((count by (instance,gpu) (DCGM_FI_DEV_GPU_UTIL)
                    unless runai_gpus_running_with_pod) - 1)
              record: runai_gpus_is_running_with_pod
            - expr: sum without (pod_ip, instance)((label_replace(runai_gpus_is_running_with_pod, "pod_ip", "$1", "instance", "(.*):(.*)")) * on(pod_ip) group_left(node) kube_pod_info{created_by_name=~"runai-dcgm-exporter"})
              record: runai_gpus_is_running_with_pod2
            - expr:  sum by ( gpu, node, pod_name, pod_namespace) ((label_replace(DCGM_FI_DEV_GPU_UTIL, "pod_ip", "$1", "instance", "(.*):(.*)")) * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~"runai-dcgm-exporter"})
              record: runai_node_gpu_utilization
            - expr: (label_replace(DCGM_FI_DEV_FB_TOTAL, "pod_ip", "$1", "instance", "(.*):(.*)")) * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~"runai-dcgm-exporter"}
              record: runai_node_gpu_total_memory_excluding_mig
            - expr: sum by(node_name, node, gpu) (runai_mig_gpu_total_memory OR label_replace(runai_node_gpu_total_memory_excluding_mig, "node_name", "$0", "node", ".*"))
              record: runai_node_gpu_total_memory
            - expr: (label_replace(DCGM_FI_DEV_FB_USED, "pod_ip", "$1", "instance", "(.*):(.*)")) * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~"runai-dcgm-exporter"}
              record: runai_node_gpu_used_memory_excluding_mig
            - expr: sum by(node_name, node, gpu) (runai_mig_gpu_used_memory OR label_replace(runai_node_gpu_used_memory_excluding_mig, "node_name", "$0", "node", ".*"))
              record: runai_node_gpu_used_memory
            - expr: timestamp(DCGM_FI_DEV_GPU_UTIL > 2) or runai_gpu_last_active_time_info_per_pod or DCGM_GPU_LAST_NOT_IDLE_TIME or timestamp(DCGM_FI_DEV_GPU_UTIL)
              record: runai_gpu_last_active_time_info_per_pod
            - expr: max by (UUID, container, device, endpoint, gpu, instance, modelName, service) (runai_gpu_last_active_time_info_per_pod)
              record: runai_gpu_last_active_time_info
            - expr: sum without (device, endpoint, instance, job, namespace, pod, pod_ip, service)((label_replace(runai_gpu_last_active_time_info, "pod_ip", "$1", "instance", "(.*):(.*)")) * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~"runai-dcgm-exporter"})
              record: runai_node_gpu_last_not_idle_time
            - expr: ((sum by (pod_group_uuid) (runai_pod_group_gpu_utilization_by_gpu)) / (count by (pod_group_uuid) (runai_pod_group_gpu_utilization_by_gpu)))
                    * on (pod_group_uuid) group_left(queue_name, workload_name, project, job_name, job_uuid) (runai_pod_group_phase_with_info{phase="Running"}==1)
              record: runai_pod_group_gpu_utilization
            - expr: kube_pod_container_info{namespace="runai"}
              record: runai_pod_container_info
            - expr: kube_pod_status_phase{namespace="runai"}==1
              record: runai_pod_status_phase
            - expr: count(label_replace(sum(runai_pod_phase_with_info{gpu_index=~"^[0-9]+$", phase="Running"}==1) by(node_name, node, gpu_index, gpu), "node", "$1", "node_name", "(.+)")) by (node, gpu_index, gpu) or sum (runai_gpus_is_running_with_pod2) by (node) * 0
              record: runai_used_shared_gpu_per_node
            - expr: avg by (pod_group_uuid, gpu, node_name, node)
                    (label_replace((runai_pod_phase_with_info{gpu_index=~"^[0-9]+$", phase="Running"} ==1), "gpu", "$1", "gpu_index", "(.+)")
                    * on(node_name, node, gpu) group_left() label_replace(runai_node_gpu_utilization, "node_name", "$1", "node", "(.+)"))
              record: runai_utilization_shared_gpu_jobs
            - expr: sum by(pod_group_uuid, job_name, pod_group_uuid, job_uuid, gpu, node_name, node) ((runai_pod_phase_with_info{phase="Running"} ==1)
                    * on (pod_name, pod_namespace) group_right(pod_uuid, pod_group_uuid, workload_name, node_name, node, job_name, job_uuid, project) DCGM_FI_DEV_GPU_UTIL)
              record: runai_utilization_full_gpu_jobs
            - expr: (runai_utilization_full_gpu_jobs or runai_utilization_shared_gpu_jobs)
              record: runai_pod_group_gpu_utilization_by_gpu
            # GPU memory
            - expr: sum by(pod_group_uuid, job_uuid, gpu, node_name, node, job_name) ((runai_pod_phase_with_info{phase="Running"}
                == 1) * on(pod_name, pod_namespace) group_right(pod_uuid, pod_group_uuid, workload_name, node_name, node, job_name, job_uuid) DCGM_FI_DEV_FB_USED)
              record: runai_used_full_gpu_memory_jobs
            - expr: avg by(pod_group_uuid, job_uuid, gpu, node_name, node, job_name) (label_replace((runai_pod_phase_with_info{gpu_index=~"^[0-9]+$",phase="Running"}
                == 1), "gpu", "$1", "gpu_index", "(.+)") * on(node_name, node,
                gpu) group_left() label_replace(runai_node_gpu_used_memory, "node_name", "$1", "node", "(.+)"))
              record: runai_used_shared_gpu_memory_jobs
            - expr: (runai_used_full_gpu_memory_jobs or runai_used_shared_gpu_memory_jobs)
              record: runai_pod_group_used_gpu_memory_by_gpu
            - expr: sum by(pod_group_uuid, job_uuid) (runai_pod_group_used_gpu_memory_by_gpu)
              record: runai_pod_group_used_gpu_memory
            # CPU utilization
            - expr: sum without (container, job, namespace, pod, service, endpoint, instance) (instance:node_cpu_utilisation:rate1m * on (pod, namespace) group_left(node) (kube_pod_info *0+1))
              record: runai_node_cpu_utilization
              # avg of all cores
            - expr: avg(1-  (rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m])))
              record: runai_cpu_utilization
            - expr: sum by (workload_name, pod_group_uuid, job_name, job_uuid , pod_namespace, project)
                (label_replace(label_replace(sum(rate(container_cpu_usage_seconds_total{ container!=""}[1m])) by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")
                * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid, project) (runai_pod_phase_with_info{phase="Running"} ==1))
              record: runai_job_cpu_usage
            - expr: rate(container_cpu_usage_seconds_total{container=""}[1m]) * on (pod, namespace) group_left(gpu, gpu_index, workload_name, workload_type) label_replace(label_replace(runai_pod_phase_with_info{phase="Running"} == 1, "pod" , "$1", "pod_name", "(.*)"), "namespace", "$1", "pod_namespace", "(.*)")
              record: runai_pod_cpu_usage
            # CPU Memory utilization
            - expr: sum without (container, device, endpoint, instance, job, namespace, pod, pod_ip, service) (instance:node_memory_utilisation:ratio * on (pod, namespace) group_left(node) (kube_pod_info *0+1))
              record: runai_node_memory_utilization
            - expr: 1 - sum (node_memory_MemAvailable_bytes{job="node-exporter"})  / sum(node_memory_MemTotal_bytes{job="node-exporter"})
              record: runai_memory_utilization
            - expr: sum without (container, endpoint,instance, job,namespace,pod,service) ((node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"} ) * on (pod, namespace) group_left(node) kube_pod_info)
              record: runai_node_memory_used_bytes
            - expr: sum(runai_node_memory_used_bytes)
              record: runai_memory_used_bytes
            - expr: sum(label_replace(label_replace(sum(container_memory_usage_bytes{ container!=""}) by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)") * on(pod_namespace, pod_name) group_left(workload_name, pod_group_uuid, job_name, job_uuid, project) (runai_pod_info * 0 + 1)) by (workload_name, pod_group_uuid, job_name, job_uuid, project)
              record: runai_job_memory_used_bytes
            - expr: max(max( label_replace(label_replace(kube_pod_container_info ,"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name, pod_namespace, image) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (image, workload_name, pod_group_uuid , job_name, job_uuid)
              record: runai_pod_group_image
            - expr: container_memory_usage_bytes{container=""} * on (pod, namespace) group_left(gpu, gpu_index, workload_name, workload_type) label_replace(label_replace(runai_pod_phase_with_info{phase="Running"} == 1, "pod" , "$1", "pod_name", "(.*)"), "namespace", "$1", "pod_namespace", "(.*)")
              record: runai_pod_memory_used_bytes
            # CPU Memory requested - note: job can be allocated but pending(ErrorImagePull)
            - expr: sum(kube_pod_container_resource_requests_memory_bytes{node!=""} * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1)) by (node)
              record: runai_node_requested_memory_bytes
            - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests_memory_bytes * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Running"}==1), "pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid, job_name, job_uuid)
              record: runai_active_job_memory_allocated_bytes
            - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests_memory_bytes * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1), "pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid, job_name, job_uuid)
              record: runai_active_job_memory_requested_bytes
            # CPU requested
            - expr: sum(kube_pod_container_resource_requests_cpu_cores{node!=""} * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1)) by (node)
              record: runai_node_cpu_requested
            - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests_cpu_cores * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Running"}==1),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid, job_name, job_uuid)
              record: runai_active_job_cpu_allocated_cores
            - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests_cpu_cores * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid, job_name, job_uuid)
              record: runai_active_job_cpu_requested_cores
            # CPU & Memory limits
            - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_limits{resource="cpu"} * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid, job_name, job_uuid)
              record: runai_active_job_cpu_limits
            - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_limits{resource="memory"} * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid, job_name, job_uuid)
              record: runai_active_job_memory_limits
            # Adapter for DCGM old metrics, please use DCGM_FI_DEV_GPU_UTIL rather than dcgm_gpu_utilization
            - expr: DCGM_FI_DEV_GPU_UTIL
              record: dcgm_gpu_utilization
            - alert: RunaiAgentPullRateLow
              expr: ((rate(successful_backend_sync_count{type="pull"}[5m])) or (absent(successful_backend_sync_count{type="pull"}) * 0)) < 0.09
              for: 10m
              labels:
                severity: critical 
              annotations:
                summary: Runai Agent fails pulling configuration from the backend (instance {{ $labels.instance }})
            - alert: RunaiAgentPushRateLow
              expr: ((rate(successful_backend_sync_count{type="push"}[5m])) or (absent(successful_backend_sync_count{type="push"}) * 0)) < 0.09
              for: 10m
              labels:
                severity: critical 
              annotations:
                summary: Runai Agent fails pushing configuration from the backend (instance {{ $labels.instance }})
            - alert: RunaiDeploymentInsufficientReplicas
              annotations:
                summary: Runai Deployment has not matched the expected number of replicas.
              expr: sum(min_over_time(kube_deployment_status_replicas_available{namespace=~"runai|runai-backend"}[30s])) < sum(min_over_time(kube_deployment_spec_replicas{namespace=~"runai|runai-backend"}[30s]))
              for: 5m
              labels:
                severity: critical
            - alert: RunaiProjectControllerReconcileFailure
              annotations:
                summary: Runai project controller service had a runtime error while reconciling
              expr: increase(controller_runtime_reconcile_errors_total{controller="project"}[10m]) > 0
              labels:
                severity: critical
            - alert: RunaiStatefulSetInsufficientReplicas
              annotations:
                summary: Runai StatefulSet has not matched the expected number of replicas.
              expr: sum(min_over_time(kube_statefulset_status_replicas_ready{namespace=~"runai|runai-backend"}[30s])) < sum(min_over_time(kube_statefulset_status_replicas{namespace=~"runai|runai-backend"}[30s]))
              for: 5m
              labels:
                severity: critical
            - alert: RunaiCriticalProblem
              annotations:
                summary: Runai platform has a critical problem
              expr: sum(ALERTS{alertname=~"Runai.*", severity="critical"} unless ALERTS{alertname="RunaiCriticalProblem"})> 0
              for: 5m
              labels:
                severity: critical
            - alert: RunaiDaemonSetRolloutStuck
              annotations:
                description: "Runai DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 5 minutes."
                summary: Runai DaemonSet rollout is stuck.
              expr: |
                (
                  (
                    kube_daemonset_status_current_number_scheduled{namespace=~"runai|runai-backend"}
                     !=
                    kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"}
                  ) or (
                    kube_daemonset_status_number_misscheduled{namespace=~"runai|runai-backend"}
                     !=
                    0
                  ) or (
                    kube_daemonset_updated_number_scheduled{namespace=~"runai|runai-backend"}
                     !=
                    kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"}
                  ) or (
                    kube_daemonset_status_number_available{namespace=~"runai|runai-backend"}
                     !=
                    kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"}
                  )
                ) and (
                  changes(kube_daemonset_updated_number_scheduled{namespace=~"runai|runai-backend"}[5m])
                    ==
                  0
                )

            # Rules renaming for customers
            - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_active_job_cpu_limits)
              record: runai_running_job_cpu_limit_cores
            - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_active_job_cpu_requested_cores)
              record: runai_running_job_cpu_requested_cores
            - expr: sum without(pod_group_name, pod_group_uuid, workload_name)(runai_active_job_memory_limits)
              record: runai_running_job_memory_limit_bytes
            - expr: sum without(pod_group_name, pod_group_uuid, workload_name)(runai_active_job_memory_requested_bytes)
              record: runai_running_job_memory_requested_bytes
            - expr: sum without(container, endpoint,instance, job,namespace,pod,service, pod_group_name, pod_group_uuid, queue_name, workload_name, workload_type)(runai_allocated_gpus)
              record: runai_job_allocated_gpus
            - expr:  sum without(pod_group_name, pod_group_uuid) (runai_cpu_utilization)
              record: runai_cluster_cpu_utilization
            - expr: sum without(instance, pod_ip)(runai_gpus_is_running_with_pod2)
              record: runai_gpu_is_allocated
            - expr: sum without(pod_group_name, pod_group_uuid, pod_namespace, workload_name)(runai_job_cpu_usage)
              record: runai_running_job_cpu_used_cores
            - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_job_memory_used_bytes)
              record: runai_running_job_memory_used_bytes
            - expr: (runai_memory_used_bytes)
              record: runai_cluster_memory_used_bytes
            - expr: (runai_memory_utilization)
              record: runai_cluster_memory_utilization
            - expr: (runai_node_cpu_requested)
              record: runai_node_cpu_requested_cores
            - expr:  sum without(UUID, Hostname, container, container_name, device, endpoint, exported_container, exported_namespace, exported_pod, instance, modelName, namespace, pod, pod_ip, pod_name, pod_namespace, service)(runai_node_gpu_last_not_idle_time)
              record: runai_gpu_last_active_time
            - expr: sum without(node_name)(runai_node_gpu_total_memory)
              record: runai_node_total_memory_bytes
            - expr: sum without(container, endpoint, instance, job, namespace, pod, service, exporter_container) (runai_node_memory_used_bytes)
              record: runai_node_used_memory_bytes
            - expr: sum without(pod_group_uuid, queue_name, workload_name)(runai_pod_group_gpu_utilization)
              record: runai_job_gpu_utilization
            - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_pod_group_image)
              record: runai_job_image
            - expr: sum without(pod_group_uuid, queue_name, workload_name, workload_type, job_uuid, node_name, phase)(runai_pod_group_phase_with_info)
              record: runai_job_status_with_info
            - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_uuid,service)(runai_pod_group_total_runtime)
              record: runai_job_total_runtime
            - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_uuid,service)(runai_pod_group_total_wait_time)
              record: runai_job_total_wait_time
            - expr: sum without(pod_group_uuid)(runai_pod_group_used_gpu_memory)
              record: runai_job_used_gpu_memory_bytes
            - expr: sum without(pod_group_uuid, node_name)(runai_pod_group_used_gpu_memory_by_gpu)
              record: runai_job_used_gpu_memory_bytes_with_gpu_node
            - expr: sum without(endpoint, instance, job,namespace, pod, queue_name,service)(runai_queue_deserved_gpus)
              record: runai_project_guaranteed_gpus
            - expr: sum without(endpoint, instance, job,namespace, pod, queue_name, service, deserved_cpu, deserved_gpu, deserved_memory)(runai_queue_info)
              record: runai_project_info
            - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_name, pod_group_uuid,queue_name,service, workload_name, workload_type)(runai_requested_gpus)
              record: runai_job_requested_gpus
            - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_name, pod_group_uuid,queue_name, service, workload_name, workload_type)(runai_requested_gpus_memory)
              record: runai_job_requested_gpu_memory
            - expr: sum without(gpu_index)(runai_used_shared_gpu_per_node)
              record: runai_gpu_is_running_fractional_job
            - expr: sum without(pod_group_uuid, node_name)(runai_utilization_full_gpu_jobs)
              record: runai_gpu_utilization_non_fractional_jobs
            - expr: sum without(node_name)(runai_node_gpu_used_memory)
              record: runai_node_gpu_used_memory_bytes
            - expr: runai_node_gpu_utilization
              record: runai_gpu_utilization_with_pod_info
            
            # GPU Nodes
            - record: runai_node_is_ready
              expr: kube_node_status_condition{condition="Ready", status="true"}
            - record: runai_cluster_ready_gpu_node_count
              expr: count((runai_node_is_ready * on (node) runai_node_gpu_count) > 0) or vector(0)

            # Total GPUs
            - record: runai_cluster_gpu_count
              expr: sum(runai_node_gpu_count)

            # Allocated GPUs
            - record: runai_cluster_allocated_dedicated_gpu_count
              expr: sum(runai_pod_info{gpu=""} and on(pod_uuid) runai_pod_phase{phase="Running"}==1) or vector(0)
            - record: runai_cluster_allocated_shared_gpu_count
              expr: count(group(runai_pod_info{gpu!=""} and on(pod_uuid) runai_pod_phase{phase="Running"}==1) by (gpu, node)) or vector(0)
            - record: runai_cluster_allocated_gpu_count
              expr: (runai_cluster_allocated_shared_gpu_count + runai_cluster_allocated_dedicated_gpu_count) or vector(0)
            
            # Project Provisioned GPUs
            - record: runai_project_provisioned_gpu_count
              expr: sum(runai_pod_provisioned_gpu_count * on(pod_uuid) group_left(project) runai_pod_info) by(project) or (group(runai_project_info) by (project) * 0)

            # Cluster Provisioned GPUs
            - record: runai_cluster_provisioned_gpu_count
              expr: sum(runai_project_provisioned_gpu_count) or vector(0)

            # Idle GPUs
            - record: runai_gpu_is_idle
              expr: group(kube_pod_info * on(pod) group_right(node) (clamp_max(max_over_time(DCGM_FI_DEV_GPU_UTIL[5m]), 1))) by (node, gpu)
        # Group of aggregate metrics which should be sampled frequently (the default interval is 10s).
        - name: runai-rules-short-interval
          interval: 1s
          rules:
            - expr: sum_over_time(avg(runai_pod_group_info{detailed_status="Running"}) by (workload_name, job_name, endpoint, instance, job, job_uuid, namespace, pod, pod_group_uuid, service)[1s:100ms]) / 10
              record: runai_pod_group_runtime_per_second
            - expr: runai_pod_group_run_time_per_second + (runai_pod_group_total_runtime or runai_pod_group_run_time_per_second * 0)
              record: runai_pod_group_total_runtime
            - expr: sum_over_time(avg(runai_pod_group_info{detailed_status=~"Pending|Swapped"}) by (workload_name, job_name, endpoint, instance, job, job_uuid, namespace, pod, pod_group_uuid, service)[1s:100ms]) / 10
              record: runai_pod_group_wait_time_per_second
            - expr: runai_pod_group_wait_time_per_second + (runai_pod_group_total_wait_time or runai_pod_group_wait_time_per_second * 0)
              record: runai_pod_group_total_wait_time
  alertmanager:
    enabled: true

  grafana:
    enabled: false

  prometheus:
    prometheusSpec:
      serviceMonitorSelector:
        matchExpressions:
        - key: release
          operator: In
          values:
          - runai
          - runai-cluster
      podMonitorNamespaceSelector:
        matchLabels:
          name: runai
      podMonitorSelector:
        matchExpressions:
        - key: release
          operator: In
          values:
          - runai
          - runai-cluster
      serviceMonitorSelectorNilUsesHelmValues: false
      scrapeInterval: 10s
      evaluationInterval: 10s
      externalLabels:
        prometheus: ""
        prometheus_replica: ""
        clusterId: ""
      remoteWrite:
        - url: https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push
          remoteTimeout: 30s
          writeRelabelConfigs:
          - sourceLabels:
            - __name__
            regex: <comes from backend/RunAIConfigProvider>
            action: keep
          basicAuth:
            username:
              name: runai-grafana-secret
              key: grafanaLabUsername
            password:
              name: runai-grafana-secret
              key: grafanaLabPassword

  prometheus-node-exporter:
    namespaceOverride: monitoring
    service:
      port: 9107
      targetPort: 9107

  kube-state-metrics:
    namespaceOverride: monitoring
    collectors:
      - daemonsets
      - deployments
      - nodes
      - pods

    # whitelist: "kube_job_created,kube_node_status_condition,kube_node_status_capacity,kube_pod_container_resource_requests_cpu_cores,kube_node_status_allocatable,kube_node_status_capacity_cpu_cores,kube_node_status_capacity_memory_bytes,kube_pod_info,kube_pod_status_phase,kube_pod_container_info,kube_pod_container_resource_requests,kube_pod_container_resource_limits,kube_pod_init_container_resource_requests,kube_node_status_allocatable_cpu_cores,kube_node_status_allocatable_memory_bytes,kube_pod_container_resource_requests_memory_bytes"

fluentd:
  enabled: false
  aggregator:
    configMapFiles:
      fluentd-output.conf: |
        <match **>
          @type elasticsearch
          hosts https://6e0ec49e030e4c9a865de9d8666eb9cc.us-central1.gcp.cloud.es.io:9243
          user write_only_user
          password Password1
          index_name "#{ENV['ENVIRONMENT']}"
          type_name fluentd
          include_timestamp true
        </match>
  forwarder:
    configMapFiles:
      fluentd-inputs.conf: |-
        # HTTP input for the liveness and readiness probes
        <source>
          @type http
          port 9880
        </source>
        <source>
          @type tail
          read_from_head true
          tag kubernetes.*
          path /var/log/containers/*_runai_*.log,/var/log/containers/*_kube_system_*.log
          pos_file /var/log/fluentd-containers.log.pos
          <parse>
            @type kubernetes
            @type "#{ENV['FLUENT_CONTAINER_TAIL_PARSER_TYPE'] || 'json'}"
            time_format %Y-%m-%dT%H:%M:%S.%NZ
          </parse>
        </source>

        <filter kubernetes.**>
          @type kubernetes_metadata
          @id filter_kube_metadata
          kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
          verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
          ca_file "#{ENV['KUBERNETES_CA_FILE']}"
          skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
          skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
          skip_master_url "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
          skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
        </filter>

        <filter kubernetes.**>
          @type record_transformer
          <record>
            tenant "#{ENV['TENANT_NAME']}"
            cluster "#{ENV['CLUSTER_NAME']}"
          </record>
        </filter>
---
